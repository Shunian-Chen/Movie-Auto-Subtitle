{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spleeter-gpu\n",
    "import os\n",
    "import gc\n",
    "import pydub\n",
    "import wave as we\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "# from spleeter.separator import Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio(videos_file_path, file_name, export_path, format = \"wav\"):\n",
    "    try:\n",
    "        audio_name = file_name.split(\".\")[0]\n",
    "        video_path = os.path.join(videos_file_path, file_name)\n",
    "        audio_path = os.path.join(export_path, audio_name) + f\".{format}\"\n",
    "        cmd = f'ffmpeg -i \\\"{video_path}\\\" -f \\\"{format}\\\" -vn -ac 1 -ar {SAMPLE_RATE} -y \\\"{audio_path}\\\"'\n",
    "        subprocess.call(cmd)\n",
    "    except Exception as ex:\n",
    "        print(\"Error: \", ex)\n",
    "    \n",
    "    return audio_name + f\".{format}\"\n",
    "\n",
    "def normalize(audio_path, audio_file, output_path, format = \"wav\"):\n",
    "    try:\n",
    "        audio_name = audio_file.split(\".\")[0]\n",
    "        input_path = os.path.join(audio_path, audio_file)\n",
    "        output_path = f\"{os.path.join(output_path, audio_name)}-normalized.{format}\"\n",
    "        cmd = f'ffmpeg-normalize \\\"{input_path}\\\" -o \\\"{output_path}\\\" --sample-rate {SAMPLE_RATE}'\n",
    "        subprocess.call(cmd)\n",
    "    except Exception as ex:\n",
    "        print(\"Error: \", ex)\n",
    "\n",
    "def vocal_spearation(audio_path, output_path):\n",
    "    try:\n",
    "        cmd = f'python inference.py --input \\\"{audio_path}\\\" --output \\\"{output_path}\\\" --gpu 0 -B 4'\n",
    "        subprocess.call(cmd)\n",
    "    except Exception as ex:\n",
    "        print(\"Error: \", ex)\n",
    "\n",
    "# def vocal_separation(audio_path, audio_name, output_path = None):\n",
    "#     if output_path == None:\n",
    "#         output_path = audio_path\n",
    "\n",
    "#     separator = Separator('spleeter:2stems')\n",
    "#     separator.separate_to_file(os.path.join(audio_path, audio_name), output_path)\n",
    "# vocal_separation(DATA_PATH, audio, OUTPUT_PATH)\n",
    "\n",
    "def split_audio(audio_path, audio_name, save_path, min_silence_len = 1000, silence_thresh = -16, keep_silence = 100):\n",
    "    #read audio\n",
    "    audio_type = os.path.splitext(audio_name)[-1][1:]\n",
    "    audio = AudioSegment.from_file(os.path.join(audio_path, audio_name), format = audio_type)\n",
    "\n",
    "    #normalize audio\n",
    "    normalized_audio = match_target_amplitude(audio, silence_thresh)\n",
    "\n",
    "    #create folder to store the result segments if not exist\n",
    "    folder = os.path.join(audio_path, f\"{min_silence_len} {silence_thresh}\")\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "    \n",
    "    #split segment\n",
    "    not_silence_ranges = pydub.silence.detect_nonsilent(normalized_audio, min_silence_len = min_silence_len, silence_thresh = silence_thresh, seek_step = 1)\n",
    "\n",
    "    #cut the slice from the original audio and save it\n",
    "    for idx in range(len(not_silence_ranges)):\n",
    "        current_start_pos = max(0, not_silence_ranges[idx][0] - keep_silence)\n",
    "        # current_end_pos = round(not_silence_ranges[idx][1])\n",
    "        current_end_pos = round(not_silence_ranges[idx][1]) if idx == len(not_silence_ranges)-1 else not_silence_ranges[idx + 1][0]-keep_silence\n",
    "\n",
    "        new = audio[current_start_pos:current_end_pos] \n",
    "\n",
    "        #segment is too small\n",
    "        if len(new) <= 500:\n",
    "            continue\n",
    "        \n",
    "        #name the segment using its time\n",
    "        file_name = f\"{current_start_pos}_{current_end_pos}.{audio_type}\"\n",
    "        save_name = os.path.join(folder, file_name)\n",
    "        new.export(save_name, format = audio_type)\n",
    "    audio = audio.empty()\n",
    "    #plot the splits on the graph\n",
    "    save_path = f\"{save_path}\\\\{min_silence_len} {silence_thresh}.png\"\n",
    "    create_plot(os.path.join(audio_path, audio_name), not_silence_ranges, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util functions\n",
    "\n",
    "def time_trans(t):\n",
    "    h = t//3600\n",
    "    m = t//60\n",
    "    s = int(t%60)\n",
    "    ms = round(t - int(t), 3) * 1000\n",
    "    return \"%02d-%02d-%02d,%03d\" % (h, m, s, ms)\n",
    "\n",
    "#adjust target amplitude\n",
    "def match_target_amplitude(sound, target_dBFS):\n",
    "    change_in_dBFS = target_dBFS - sound.dBFS\n",
    "    return sound.apply_gain(change_in_dBFS)\n",
    "    \n",
    "def wavread(path):\n",
    "    wavfile =  we.open(path,\"rb\")\n",
    "    params = wavfile.getparams()\n",
    "    framesra,frameswav= params[2],params[3]\n",
    "    nchannels, sampwidth, framesra, frameswav = params[:4]\n",
    "\n",
    "    datawav = wavfile.readframes(frameswav)\n",
    "    wavfile.close()\n",
    "    datause = np.frombuffer(datawav,dtype = np.short)\n",
    "\n",
    "    if nchannels == 2:\n",
    "        datause.shape = -1,2\n",
    "        datause = datause[:, 0]\n",
    "    datause = datause.T\n",
    "    time = np.arange(0, frameswav) * (1.0/framesra)\n",
    "    return datause,time,nchannels\n",
    "\n",
    "def create_plot(path, not_silence_ranges, save_path):\n",
    "    wavdata,wavtime,_ = wavread(path)\n",
    "\n",
    "    #plot the sound wave\n",
    "    plt.figure(figsize=(120, 20))\n",
    "    plt.plot(wavtime,wavdata,color = 'green')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "\n",
    "    #plot the split points\n",
    "    split_points = np.array(not_silence_ranges).flatten()/1000\n",
    "    for i in range(len(split_points)):\n",
    "        if i%2==0:\n",
    "            plt.axvline(split_points[i], color = \"red\")\n",
    "        else:\n",
    "            plt.axvline(split_points[i], color = \"blue\")\n",
    "\n",
    "    plt.savefig(save_path)\n",
    "    plt.clf()\n",
    "    plt.close(\"all\")\n",
    "    del wavdata, wavtime\n",
    "    gc.collect()\n",
    "    # plt.show()\n",
    "    \n",
    "\n",
    "def test_normalize_first(audio_file):\n",
    "    audio_name = audio_file.split('.')[0]\n",
    "    audio_format = audio_file.split('.')[1]\n",
    "\n",
    "    nor_sep_folder = os.path.join(AUDIO_PATH, \"Normalized\")\n",
    "    nor_sep_fig = os.path.join(FIG_PATH, \"Normalized\")\n",
    "    if not os.path.exists(nor_sep_folder):\n",
    "        os.mkdir(nor_sep_folder)\n",
    "    if not os.path.exists(nor_sep_fig):\n",
    "        os.mkdir(nor_sep_fig)\n",
    "\n",
    "    print(\"Normalizing...\")\n",
    "    normalize(AUDIO_PATH, audio_file, nor_sep_folder, audio_format)\n",
    "    print(\"Done\")\n",
    "\n",
    "    print(\"Separating...\")\n",
    "    normalized_name = f\"{audio_name}-normalized.{audio_format}\"\n",
    "    audio_path = os.path.join(nor_sep_folder, normalized_name)\n",
    "    vocal_spearation(audio_path, nor_sep_folder)\n",
    "    print(\"Done\")\n",
    "\n",
    "    print(\"Spliting\")\n",
    "    normalized_vocal_name = f\"{audio_name}-normalized_Vocals.{audio_format}\"\n",
    "    for min_silence in MIN_SILENCE_LEN:\n",
    "        print(\"Spliting with minimum silence: \", min_silence)\n",
    "        for sil_thresh in SILENCE_THRESH:\n",
    "            print(\"silence threshold: \", sil_thresh)\n",
    "            split_audio(nor_sep_folder, normalized_vocal_name, nor_sep_fig, min_silence_len = min_silence, silence_thresh = sil_thresh)\n",
    "    print(\"Done\")\n",
    "\n",
    "def test_voc_first(audio_file):\n",
    "    audio_name = audio_file.split('.')[0]\n",
    "    audio_format = audio_file.split('.')[1]\n",
    "    \n",
    "    sep_nor_folder = os.path.join(AUDIO_PATH, \"Vocal\")\n",
    "    sep_nor_fig = os.path.join(FIG_PATH, \"Vocal\")\n",
    "    if not os.path.exists(sep_nor_folder):\n",
    "        os.mkdir(sep_nor_folder)\n",
    "    if not os.path.exists(sep_nor_fig):\n",
    "        os.mkdir(sep_nor_fig)\n",
    "    audio_path = os.path.join(AUDIO_PATH, audio_file)\n",
    "\n",
    "    print(\"Separating...\")\n",
    "    vocal_spearation(audio_path, sep_nor_folder)\n",
    "    print(\"Done\")\n",
    "\n",
    "    print(\"Normalizing...\")\n",
    "    voc_file = f\"{audio_name}_Vocals.{audio_format}\"\n",
    "    normalize(sep_nor_folder, voc_file, sep_nor_folder, audio_format)\n",
    "    voc_normalized_name = f\"{audio_name}_Vocals-normalized.{audio_format}\"\n",
    "    print(\"Done\")\n",
    "\n",
    "    print(\"Spliting\")\n",
    "    for min_silence in MIN_SILENCE_LEN:\n",
    "        print(\"Spliting with minimum silence: \", min_silence)\n",
    "        for sil_thresh in SILENCE_THRESH:\n",
    "            print(\"silence threshold: \", sil_thresh)\n",
    "            split_audio(sep_nor_folder, voc_normalized_name, sep_nor_fig, min_silence_len = min_silence, silence_thresh = sil_thresh)\n",
    "    print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\Data\\\\videos'\n",
    "AUDIO_PATH = 'E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\Data\\\\outputs'\n",
    "FIG_PATH = 'E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\imgs\\\\split'\n",
    "SAMPLE_RATE = 16000\n",
    "MIN_SILENCE_LEN = [1500]\n",
    "SILENCE_THRESH = [-70]\n",
    "video_name = \"return1.mp4\"\n",
    "audio_format = \"wav\"\n",
    "\n",
    "def test():\n",
    "    print(\"Extracting audio..\")\n",
    "    audio_name = extract_audio(DATA_PATH, video_name, AUDIO_PATH, audio_format)\n",
    "    print(\"Done\")\n",
    "\n",
    "    # print(\"Testing normalized first..\")\n",
    "    # #First normalize then separate vocal\n",
    "    # test_normalize_first(audio_name)\n",
    "    # print(\"Test Done\")\n",
    "\n",
    "    print(\"Testing vocal separation first..\")\n",
    "    #First separate vocal then normalize\n",
    "    test_voc_first(audio_name)\n",
    "    print(\"Test Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio..\n",
      "Done\n",
      "Testing vocal separation first..\n",
      "Separating...\n",
      "Done\n",
      "Normalizing...\n",
      "Done\n",
      "Spliting\n",
      "Spliting with minimum silence:  1500\n",
      "silence threshold:  -70\n",
      "Done\n",
      "Test Done\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e59ba9495cf5a0f7daa13e34e418c9fb6692db49850371a2f353821d5213ce6c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ind')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
