{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "import os\n",
    "import re\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_metric, Dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.path.abspath(os.path.join(os.path.abspath(\".\"), \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"K:\\\\AIPI540\\\\Individual Project\\\\cv-corpus-8.0-2022-01-19-en\\\\cv-corpus-8.0-2022-01-19\\\\en\\\\clips\"\n",
    "LABEL_PATH = \"K:\\\\AIPI540\\\\Individual Project\\\\cv-corpus-8.0-2022-01-19-en\\\\cv-corpus-8.0-2022-01-19\\\\en\"\n",
    "AUDIO_BASE = \"E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\Data\\\\Train\"\n",
    "VOCAL_PATH = \"E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\Data\\\\Train\\\\Vocal\"\n",
    "CLEAN_AUDIO = \"E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\Data\\\\Train\\\\Clean\"\n",
    "SIZE = 20000\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ds, test_size = 0.3):\n",
    "    n = len(ds)\n",
    "    idx = np.random.permutation(n)\n",
    "    train = ds.select(idx[round(n*test_size):])\n",
    "    test = ds.select(idx[:round(n*test_size)])\n",
    "    return train, test\n",
    "\n",
    "def label_preprocess(line):\n",
    "    line['path'] = os.path.join(CLEAN_AUDIO, line['path'].split(\".\")[0]+\".flac\")\n",
    "    line['sentence'] = re.sub(r'[^A-Za-z0-9 ]+',\"\",line['sentence']).upper()\n",
    "    return line\n",
    "# labels = pd.read_csv(os.path.join(LABEL_PATH, \"train.tsv\"), sep = \"\\t\").loc[:SIZE, ['path', 'sentence']]\n",
    "# processed_labels = labels.copy().apply(label_preprocess, axis = 1)\n",
    "# processed_labels.head()\n",
    "    \n",
    "def normalize(audio_path, audio_file, output_path, format = \"wav\"):\n",
    "    try:\n",
    "        audio_name = audio_file.split(\".\")[0]\n",
    "        input_path = os.path.join(audio_path, audio_file)\n",
    "        output_path = f\"{os.path.join(output_path, audio_name)}-normalized.{format}\"\n",
    "        cmd = f'ffmpeg-normalize \\\"{input_path}\\\" -o \\\"{output_path}\\\"'\n",
    "        subprocess.call(cmd)\n",
    "    except Exception as ex:\n",
    "        print(\"Error: \", ex)\n",
    "\n",
    "def vocal_spearation(audio_path, output_path):\n",
    "    try:\n",
    "        cmd = f'python inference.py --input \\\"{audio_path}\\\" --output \\\"{output_path}\\\" --gpu 0 -B 4'\n",
    "        subprocess.call(cmd)\n",
    "    except Exception as ex:\n",
    "        print(\"Error: \", ex)\n",
    "\n",
    "def trans2flac(audio_path, audio_name, output_path, format):\n",
    "    audio_type = audio_name.split(\".\")[-1][1:]\n",
    "    exp_file = audio_name.split(\".\")[0]\n",
    "    audio = AudioSegment.from_file(os.path.join(audio_path, audio_name), format=audio_type)\n",
    "    audio.export(f'{os.path.join(output_path, exp_file)}.{format}', format = str(format))\n",
    "    return f\"{exp_file}.{format}\"\n",
    "\n",
    "# for idx, row in data.iterrows():\n",
    "#     trans2flac(VOCAL_PATH, row['path'], CLEAN_AUDIO, 'flac')\n",
    "\n",
    "def audio_preprocess(audio_file):\n",
    "    audio_name = audio_file.split('.')[0]\n",
    "    audio_format = 'wav'\n",
    "\n",
    "    nor_sep_folder = os.path.join(AUDIO_BASE, \"Normalized\")\n",
    "    if not os.path.exists(nor_sep_folder):\n",
    "        os.mkdir(nor_sep_folder)\n",
    "\n",
    "    print(\"Normalizing...\")\n",
    "    normalize(DATA_PATH, audio_file, nor_sep_folder, audio_format)\n",
    "    print(\"Done\")\n",
    "\n",
    "    print(\"Separating...\")\n",
    "    normalized_name = f\"{audio_name}-normalized.{audio_format}\"\n",
    "    audio_path = os.path.join(nor_sep_folder, normalized_name)\n",
    "    vocal_spearation(audio_path, VOCAL_PATH)\n",
    "    print(\"Done\")\n",
    "# labels['path'].apply(audio_preprocess)\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # audio = batch[\"audio\"]\n",
    "    audio_input, sample_rate = sf.read(batch[\"path\"])\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio_input, sampling_rate=sample_rate).input_values[0]\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "def extract_audio(path, export_path = \"E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\Data\\\\Train\\\\Normalized\", format = \"wav\"):\n",
    "    try:\n",
    "        file_name = path.split(\"\\\\\")[-1]\n",
    "        audio_name = file_name.split(\".\")[0]\n",
    "        audio_path = os.path.join(export_path, audio_name) + f\".{format}\"\n",
    "        cmd = f'ffmpeg -i \\\"{path}\\\" -f \\\"{format}\\\" -vn -ac 1 -ar {SAMPLE_RATE} -y \\\"{audio_path}\\\"'\n",
    "        subprocess.call(cmd)\n",
    "    except Exception as ex:\n",
    "        print(\"Error: \", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12176\\3134579225.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLABEL_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train.tsv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprocessed_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_preprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprocessed_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv(os.path.join(LABEL_PATH, \"train.tsv\"), sep = \"\\t\").loc[:SIZE, ['path', 'sentence']]\n",
    "processed_labels = labels.copy().apply(label_preprocess, axis = 1)\n",
    "processed_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_labels.to_csv(\"E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\Data\\\\Train\\\\labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K:\\AIPI540\\Individual Project\\cv-corpus-8.0-20...</td>\n",
       "      <td>THEREAFTER THE CLASS WAS HIGHLY RESPECTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K:\\AIPI540\\Individual Project\\cv-corpus-8.0-20...</td>\n",
       "      <td>BANARAS HINDU UNIVERSITY IS A CENTRAL UNIVERSI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K:\\AIPI540\\Individual Project\\cv-corpus-8.0-20...</td>\n",
       "      <td>ON DISPLAY ARE HOME FURNISHINGS PIONEER TOOLS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K:\\AIPI540\\Individual Project\\cv-corpus-8.0-20...</td>\n",
       "      <td>ELEVA AND STRUM EACH HOUSE AN ELEMENTARY SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K:\\AIPI540\\Individual Project\\cv-corpus-8.0-20...</td>\n",
       "      <td>THE EASTERN PORTION OF THE COUNTY LIES WITHIN ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  K:\\AIPI540\\Individual Project\\cv-corpus-8.0-20...   \n",
       "1  K:\\AIPI540\\Individual Project\\cv-corpus-8.0-20...   \n",
       "2  K:\\AIPI540\\Individual Project\\cv-corpus-8.0-20...   \n",
       "3  K:\\AIPI540\\Individual Project\\cv-corpus-8.0-20...   \n",
       "4  K:\\AIPI540\\Individual Project\\cv-corpus-8.0-20...   \n",
       "\n",
       "                                            sentence  \n",
       "0          THEREAFTER THE CLASS WAS HIGHLY RESPECTED  \n",
       "1  BANARAS HINDU UNIVERSITY IS A CENTRAL UNIVERSI...  \n",
       "2  ON DISPLAY ARE HOME FURNISHINGS PIONEER TOOLS ...  \n",
       "3    ELEVA AND STRUM EACH HOUSE AN ELEMENTARY SCHOOL  \n",
       "4  THE EASTERN PORTION OF THE COUNTY LIES WITHIN ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_labels.path = processed_labels['path'].apply(lambda x: os.path.join(DATA_PATH, x))\n",
    "processed_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K:\\\\AIPI540\\\\Individual Project\\\\cv-corpus-8.0-2022-01-19-en\\\\cv-corpus-8.0-2022-01-19\\\\en\\\\clips\\\\common_voice_en_28449980.mp3'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_labels['path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels = pd.read_csv(\"E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\Data\\\\Train\\\\labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           path  \\\n",
      "0  common_voice_en_28449980.mp3   \n",
      "1  common_voice_en_28449981.mp3   \n",
      "2  common_voice_en_28449984.mp3   \n",
      "3  common_voice_en_28449986.mp3   \n",
      "4  common_voice_en_20293200.mp3   \n",
      "\n",
      "                                            sentence  \n",
      "0         Thereafter the class was highly respected.  \n",
      "1  Banaras Hindu University is a Central Universi...  \n",
      "2  On display are home furnishings, pioneer tools...  \n",
      "3   Eleva and Strum each house an elementary school.  \n",
      "4  The eastern portion of the county lies within ...  \n"
     ]
    }
   ],
   "source": [
    "print(labels.loc[:4, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "19996    None\n",
       "19997    None\n",
       "19998    None\n",
       "19999    None\n",
       "20000    None\n",
       "Name: path, Length: 20001, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_labels['path'].apply(extract_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "def trans2flac(audio_path, audio_name, output_path, format):\n",
    "    audio_type = 'wav'\n",
    "    exp_file = audio_name.split(\".\")[0]\n",
    "    audio = AudioSegment.from_file(os.path.join(audio_path, exp_file + '.wav'), format=audio_type)\n",
    "    audio.export(f'{os.path.join(output_path, exp_file)}.{format}', format = str(format))\n",
    "    return f\"{exp_file}.{format}\"\n",
    "LABEL_PATH = \"K:\\\\AIPI540\\\\Individual Project\\\\cv-corpus-8.0-2022-01-19-en\\\\cv-corpus-8.0-2022-01-19\\\\en\"\n",
    "SIZE = 20000\n",
    "labels = pd.read_csv(os.path.join(LABEL_PATH, \"train.tsv\"), sep = \"\\t\").loc[:SIZE, ['path', 'sentence']]\n",
    "CLEAN_AUDIO = \"E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\Data\\\\Train\\\\Clean\"\n",
    "normal_path = \"E:\\\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\Data\\\\Train\\\\Normalized\"\n",
    "labels.path.apply(lambda x: trans2flac(normal_path, x, CLEAN_AUDIO, 'flac'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_labels.path = labels.path.apply(lambda x: os.path.join(CLEAN_AUDIO, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_csv(os.path.join(AUDIO_BASE, \"label.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individua...</td>\n",
       "      <td>THEREAFTER THE CLASS WAS HIGHLY RESPECTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individua...</td>\n",
       "      <td>BANARAS HINDU UNIVERSITY IS A CENTRAL UNIVERSI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individua...</td>\n",
       "      <td>ON DISPLAY ARE HOME FURNISHINGS PIONEER TOOLS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individua...</td>\n",
       "      <td>ELEVA AND STRUM EACH HOUSE AN ELEMENTARY SCHOOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individua...</td>\n",
       "      <td>THE EASTERN PORTION OF THE COUNTY LIES WITHIN ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individua...   \n",
       "1  E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individua...   \n",
       "2  E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individua...   \n",
       "3  E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individua...   \n",
       "4  E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individua...   \n",
       "\n",
       "                                            sentence  \n",
       "0          THEREAFTER THE CLASS WAS HIGHLY RESPECTED  \n",
       "1  BANARAS HINDU UNIVERSITY IS A CENTRAL UNIVERSI...  \n",
       "2  ON DISPLAY ARE HOME FURNISHINGS PIONEER TOOLS ...  \n",
       "3    ELEVA AND STRUM EACH HOUSE AN ELEMENTARY SCHOOL  \n",
       "4  THE EASTERN PORTION OF THE COUNTY LIES WITHIN ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.path = label['path'].apply(lambda x: os.path.join(CLEAN_AUDIO, x))\n",
    "label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(\"cuda\")\n",
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4306/4306 [00:14<00:00, 295.95ex/s]\n",
      "100%|██████████| 1845/1845 [00:04<00:00, 410.08ex/s]\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataset)\n",
    "train_pro = train.map(prepare_dataset, remove_columns=train.column_names)\n",
    "test_pro = test.map(prepare_dataset, remove_columns=test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"E:\\Graduate\\\\2021-2022 Term 2\\\\AIPI540\\\\Individual Project\\\\wav2vec2-base-960h-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=2,\n",
    "  gradient_accumulation_steps=2,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=150,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=400,\n",
    "  eval_steps=400,\n",
    "  logging_steps=400,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=5,\n",
    "  push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_pro,\n",
    "    eval_dataset=test_pro,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 4306\n",
      "  Num Epochs = 150\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 161400\n",
      "  0%|          | 400/161400 [02:34<11:38:46,  3.84it/s]***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 115.4869, 'learning_rate': 0.00023459999999999998, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "  0%|          | 400/161400 [04:17<11:38:46,  3.84it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-400\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 550.9115600585938, 'eval_wer': 0.5642275551746304, 'eval_runtime': 103.1518, 'eval_samples_per_second': 17.886, 'eval_steps_per_second': 2.239, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-400\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-400\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-800] due to args.save_total_limit\n",
      "  0%|          | 800/161400 [06:48<12:38:29,  3.53it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 157.6598, 'learning_rate': 0.00029946115599751395, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "  0%|          | 800/161400 [08:27<12:38:29,  3.53it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-800\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 520.6007690429688, 'eval_wer': 0.6527748017998715, 'eval_runtime': 98.8933, 'eval_samples_per_second': 18.656, 'eval_steps_per_second': 2.336, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-800\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-800\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-900] due to args.save_total_limit\n",
      "  1%|          | 1200/161400 [11:01<17:12:36,  2.59it/s] ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 163.3341, 'learning_rate': 0.00029871535114978244, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  1%|          | 1200/161400 [12:36<17:12:36,  2.59it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-1200\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-1200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 555.0115966796875, 'eval_wer': 0.6321512749089351, 'eval_runtime': 95.2322, 'eval_samples_per_second': 19.374, 'eval_steps_per_second': 2.426, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-1200\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-1200\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-100] due to args.save_total_limit\n",
      "  1%|          | 1600/161400 [15:10<16:56:36,  2.62it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 157.2167, 'learning_rate': 0.00029797141081417024, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  1%|          | 1600/161400 [16:49<16:56:36,  2.62it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-1600\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-1600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 501.5813903808594, 'eval_wer': 0.6275980287122348, 'eval_runtime': 99.1074, 'eval_samples_per_second': 18.616, 'eval_steps_per_second': 2.331, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-1600\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-1600\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-200] due to args.save_total_limit\n",
      "  1%|          | 2000/161400 [19:26<17:32:26,  2.52it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 157.9687, 'learning_rate': 0.00029722560596643873, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  1%|          | 2000/161400 [21:06<17:32:26,  2.52it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2000\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 526.0159912109375, 'eval_wer': 0.6640775658881508, 'eval_runtime': 100.1854, 'eval_samples_per_second': 18.416, 'eval_steps_per_second': 2.306, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2000\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2000\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-300] due to args.save_total_limit\n",
      "  1%|▏         | 2400/161400 [23:48<13:45:29,  3.21it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 152.9379, 'learning_rate': 0.0002964798011187073, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  1%|▏         | 2400/161400 [25:28<13:45:29,  3.21it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2400\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 536.2674560546875, 'eval_wer': 0.658774373259053, 'eval_runtime': 99.9312, 'eval_samples_per_second': 18.463, 'eval_steps_per_second': 2.312, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2400\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2400\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-400] due to args.save_total_limit\n",
      "  2%|▏         | 2800/161400 [27:56<12:05:43,  3.64it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 147.2894, 'learning_rate': 0.0002957339962709758, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 2800/161400 [29:30<12:05:43,  3.64it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2800\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 525.5266723632812, 'eval_wer': 0.6430790657810156, 'eval_runtime': 94.6747, 'eval_samples_per_second': 19.488, 'eval_steps_per_second': 2.44, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2800\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2800\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-800] due to args.save_total_limit\n",
      "  2%|▏         | 3200/161400 [31:59<14:23:24,  3.05it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 149.9966, 'learning_rate': 0.00029499005593536357, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  2%|▏         | 3200/161400 [33:40<14:23:24,  3.05it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-3200\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-3200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 558.5145874023438, 'eval_wer': 0.6670773516177416, 'eval_runtime': 100.2476, 'eval_samples_per_second': 18.404, 'eval_steps_per_second': 2.304, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-3200\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-3200\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-1200] due to args.save_total_limit\n",
      "  2%|▏         | 3600/161400 [36:21<18:42:36,  2.34it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 142.3915, 'learning_rate': 0.00029424425108763206, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 3600/161400 [37:58<18:42:36,  2.34it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-3600\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-3600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 571.0944213867188, 'eval_wer': 0.6249196485965288, 'eval_runtime': 96.728, 'eval_samples_per_second': 19.074, 'eval_steps_per_second': 2.388, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-3600\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-3600\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-1600] due to args.save_total_limit\n",
      "  2%|▏         | 4000/161400 [40:22<16:37:04,  2.63it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 145.632, 'learning_rate': 0.00029349844623990056, 'epoch': 3.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 4000/161400 [41:57<16:37:04,  2.63it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4000\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 555.8987426757812, 'eval_wer': 0.6212234840368545, 'eval_runtime': 94.3156, 'eval_samples_per_second': 19.562, 'eval_steps_per_second': 2.449, 'epoch': 3.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4000\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4000\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2000] due to args.save_total_limit\n",
      "  3%|▎         | 4400/161400 [44:26<13:28:59,  3.23it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 140.3054, 'learning_rate': 0.00029275264139216905, 'epoch': 4.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 4400/161400 [46:04<13:28:59,  3.23it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4400\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 563.0346069335938, 'eval_wer': 0.6593636168845083, 'eval_runtime': 98.6657, 'eval_samples_per_second': 18.7, 'eval_steps_per_second': 2.341, 'epoch': 4.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4400\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4400\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2400] due to args.save_total_limit\n",
      "  3%|▎         | 4800/161400 [48:40<13:33:47,  3.21it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 141.4076, 'learning_rate': 0.0002920068365444375, 'epoch': 4.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  3%|▎         | 4800/161400 [50:19<13:33:47,  3.21it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4800\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 516.6089477539062, 'eval_wer': 0.6326333833297622, 'eval_runtime': 99.4587, 'eval_samples_per_second': 18.55, 'eval_steps_per_second': 2.323, 'epoch': 4.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4800\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4800\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-2800] due to args.save_total_limit\n",
      "  3%|▎         | 5200/161400 [52:57<13:59:38,  3.10it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 139.5069, 'learning_rate': 0.000291261031696706, 'epoch': 4.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  3%|▎         | 5200/161400 [54:39<13:59:38,  3.10it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-5200\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-5200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 535.2174072265625, 'eval_wer': 0.6372401971287766, 'eval_runtime': 102.3742, 'eval_samples_per_second': 18.022, 'eval_steps_per_second': 2.256, 'epoch': 4.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-5200\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-5200\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-3200] due to args.save_total_limit\n",
      "  3%|▎         | 5600/161400 [57:21<17:48:34,  2.43it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 137.2634, 'learning_rate': 0.0002905152268489745, 'epoch': 5.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      "  3%|▎         | 5600/161400 [59:02<17:48:34,  2.43it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-5600\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-5600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 526.1198120117188, 'eval_wer': 0.6327940861367045, 'eval_runtime': 101.7327, 'eval_samples_per_second': 18.136, 'eval_steps_per_second': 2.271, 'epoch': 5.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-5600\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-5600\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-3600] due to args.save_total_limit\n",
      "  4%|▎         | 6000/161400 [1:01:46<17:59:06,  2.40it/s]***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 132.7893, 'learning_rate': 0.000289769422001243, 'epoch': 5.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▎         | 6000/161400 [1:03:27<17:59:06,  2.40it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6000\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 571.4874877929688, 'eval_wer': 0.6337583029783587, 'eval_runtime': 101.1161, 'eval_samples_per_second': 18.246, 'eval_steps_per_second': 2.285, 'epoch': 5.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6000\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6000\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4000] due to args.save_total_limit\n",
      "  4%|▍         | 6400/161400 [1:06:07<16:41:20,  2.58it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 137.7721, 'learning_rate': 0.00028902361715351147, 'epoch': 5.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 6400/161400 [1:07:45<16:41:20,  2.58it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6400\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 693.3424072265625, 'eval_wer': 0.6747375187486608, 'eval_runtime': 97.6318, 'eval_samples_per_second': 18.898, 'eval_steps_per_second': 2.366, 'epoch': 5.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6400\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6400\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4400] due to args.save_total_limit\n",
      "  4%|▍         | 6800/161400 [1:10:18<13:41:53,  3.14it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 130.5798, 'learning_rate': 0.00028827781230577996, 'epoch': 6.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 6800/161400 [1:11:51<13:41:53,  3.14it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6800\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 556.753173828125, 'eval_wer': 0.6346153846153846, 'eval_runtime': 93.0142, 'eval_samples_per_second': 19.836, 'eval_steps_per_second': 2.483, 'epoch': 6.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6800\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6800\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-4800] due to args.save_total_limit\n",
      "  4%|▍         | 7200/161400 [1:14:09<12:36:17,  3.40it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 132.6577, 'learning_rate': 0.00028753200745804846, 'epoch': 6.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \n",
      "  4%|▍         | 7200/161400 [1:15:41<12:36:17,  3.40it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-7200\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-7200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 587.6311645507812, 'eval_wer': 0.6832011999142918, 'eval_runtime': 92.7963, 'eval_samples_per_second': 19.882, 'eval_steps_per_second': 2.489, 'epoch': 6.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-7200\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-7200\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-5200] due to args.save_total_limit\n",
      "  5%|▍         | 7600/161400 [1:18:09<16:59:47,  2.51it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 130.8342, 'learning_rate': 0.00028678620261031695, 'epoch': 7.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \n",
      "  5%|▍         | 7600/161400 [1:19:45<16:59:47,  2.51it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-7600\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-7600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 581.9589233398438, 'eval_wer': 0.6193486179558603, 'eval_runtime': 96.6271, 'eval_samples_per_second': 19.094, 'eval_steps_per_second': 2.391, 'epoch': 7.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-7600\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-7600\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-5600] due to args.save_total_limit\n",
      "  5%|▍         | 8000/161400 [1:22:15<16:27:33,  2.59it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 125.3726, 'learning_rate': 0.0002860403977625854, 'epoch': 7.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \n",
      "  5%|▍         | 8000/161400 [1:23:52<16:27:33,  2.59it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8000\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 542.4429321289062, 'eval_wer': 0.631240625669595, 'eval_runtime': 96.7248, 'eval_samples_per_second': 19.075, 'eval_steps_per_second': 2.388, 'epoch': 7.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8000\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8000\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6000] due to args.save_total_limit\n",
      "  5%|▌         | 8400/161400 [1:26:28<17:16:58,  2.46it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 127.3685, 'learning_rate': 0.00028529459291485394, 'epoch': 7.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \n",
      "  5%|▌         | 8400/161400 [1:28:04<17:16:58,  2.46it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8400\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 523.9561767578125, 'eval_wer': 0.6285086779515748, 'eval_runtime': 96.4261, 'eval_samples_per_second': 19.134, 'eval_steps_per_second': 2.396, 'epoch': 7.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8400\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8400\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6400] due to args.save_total_limit\n",
      "  5%|▌         | 8800/161400 [1:30:33<12:45:53,  3.32it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 123.0913, 'learning_rate': 0.00028454878806712244, 'epoch': 8.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \n",
      "  5%|▌         | 8800/161400 [1:32:09<12:45:53,  3.32it/s]Saving model checkpoint to E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8800\n",
      "Configuration saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 547.8412475585938, 'eval_wer': 0.6310263552603386, 'eval_runtime': 95.5429, 'eval_samples_per_second': 19.311, 'eval_steps_per_second': 2.418, 'epoch': 8.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8800\\pytorch_model.bin\n",
      "Feature extractor saved in E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-8800\\preprocessor_config.json\n",
      "Deleting older checkpoint [E:\\Graduate\\2021-2022 Term 2\\AIPI540\\Individual Project\\wav2vec2-base-960h-finetune\\checkpoint-6800] due to args.save_total_limit\n",
      "  6%|▌         | 9200/161400 [1:35:04<16:13:46,  2.60it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 1845\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 126.5893, 'learning_rate': 0.00028380484773151023, 'epoch': 8.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27044\\4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1473\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1475\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1476\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1600\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1602\u001b[1;33m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1603\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2262\u001b[0m             \u001b[0mprediction_loss_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2263\u001b[0m             \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2264\u001b[1;33m             \u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2265\u001b[0m         )\n\u001b[0;32m   2266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2419\u001b[0m         \u001b[0mobserved_num_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2420\u001b[0m         \u001b[1;31m# Main evaluation loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2421\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2422\u001b[0m             \u001b[1;31m# Update the observed num examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2423\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1764\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m         return self._getitem(\n\u001b[1;32m-> 1766\u001b[1;33m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1767\u001b[0m         )\n\u001b[0;32m   1768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m         formatted_output = format_table(\n\u001b[1;32m-> 1751\u001b[1;33m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1752\u001b[0m         )\n\u001b[0;32m   1753\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mformatted_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonArrowExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseArrowExtractor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\pyarrow\\table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.to_pydict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\pyarrow\\table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.ChunkedArray.to_pylist\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\pyarrow\\array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\pyarrow\\scalar.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.ListScalar.as_py\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\ind\\lib\\site-packages\\pyarrow\\array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e59ba9495cf5a0f7daa13e34e418c9fb6692db49850371a2f353821d5213ce6c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ind')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
